# Docker Compose configuration for Weather Data Pipeline
# Orchestrates PostgreSQL database and future services (Airflow, Streamlit)

version: '3.8'

services:
  # PostgreSQL Database Service
  # This is our data warehouse that stores raw and transformed weather data
  postgres:
    # Using Alpine variant for smaller image size (~80MB vs ~300MB for standard postgres)
    # Alpine is a minimal Linux distribution that reduces attack surface and deployment time
    image: postgres:15-alpine

    container_name: weather_postgres

    # Expose PostgreSQL on default port 5432
    # Format: host_port:container_port
    ports:
      - "5432:5432"

    # Environment variables loaded from .env file
    # These configure the initial database, user, and password
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}

    # Volume mounts for initialization and data persistence
    volumes:
      # init_db.sql runs automatically when the container first starts
      # It creates our schema (tables, indexes) for weather data storage
      # This file is executed by docker-entrypoint-initdb.d on first initialization
      - ./sql/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql

      # Named volume for persistent data storage
      # Without this, all data would be lost when the container stops
      # This ensures our weather data persists across container restarts
      - postgres_data:/var/lib/postgresql/data

    # Health check to verify PostgreSQL is ready to accept connections
    # pg_isready is a PostgreSQL utility that checks if the server is accepting connections
    # This prevents other services (like Airflow) from trying to connect before DB is ready
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}" ]
      interval: 10s # Check every 10 seconds
      timeout: 5s # Wait 5 seconds for response
      retries: 5 # Retry 5 times before marking as unhealthy
      start_period: 10s # Grace period before starting health checks

    # Always restart the container if it stops
    # Ensures database availability even after system reboots or crashes
    restart: always

    # Connect to custom network for inter-service communication
    networks:
      - weather_network

  # ============================================================================
  # REDIS SERVICE (Message Broker)
  # ============================================================================
  # Redis acts as a message broker for Airflow, handling communication 
  # between the scheduler and workers (even in LocalExecutor mode)
  redis:
    image: redis:7-alpine
    container_name: weather_redis
    ports:
      - "6379:6379"
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 30s
      retries: 50
    restart: always
    networks:
      - weather_network

  # ============================================================================
  # AIRFLOW WEBSERVER
  # ============================================================================
  # The UI interface for monitoring and triggering DAGs
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: weather_airflow_webserver
    command: webserver
    restart: always
    ports:
      - "8080:8080"

    # Dependencies ensure DB and Redis are ready before Airflow starts
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    # Environment Variables Configuration
    environment:
      # LocalExecutor runs tasks in the scheduler process (simplifies setup for learning/small scale)
      # vs CeleryExecutor which distributes tasks to separate worker nodes (complex, better scaling)
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor

      # Database connection string
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}

      # Disable loading example DAGs to keep UI clean
      - AIRFLOW__CORE__LOAD_EXAMPLES=False

      # Location of our DAGs
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags

      # Automatically migrate database schema on startup
      - _AIRFLOW_DB_MIGRATE=true

      # Encryption key (from .env)
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW_UID=${AIRFLOW_UID}

    # Volume Mounts (Sync host files to container)
    volumes:
      - ./airflow/dags:/opt/airflow/dags # DAG code
      - ./airflow/logs:/opt/airflow/logs # Task logs
      - ./airflow/plugins:/opt/airflow/plugins # Custom plugins
      - ./dbt:/opt/dbt # dbt project access
      - ./.env:/opt/airflow/.env # API keys access

    networks:
      - weather_network

  # ============================================================================
  # AIRFLOW SCHEDULER
  # ============================================================================
  # The heartbeat of Airflow - triggers tasks and schedules DAG runs
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: weather_airflow_scheduler
    command: scheduler
    restart: always

    # Needs same dependencies and env vars as webserver
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - _AIRFLOW_DB_MIGRATE=true
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW_UID=${AIRFLOW_UID}

    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./dbt:/opt/dbt
      - ./.env:/opt/airflow/.env

    networks:
      - weather_network

# Named volumes for persistent data storage
# These volumes persist even when containers are removed
volumes:
  postgres_data:
    # Stores PostgreSQL data files
    # Location on host: Docker's volume directory (managed by Docker)

    # Custom bridge network for service communication
    # Services on the same network can communicate using service names as hostnames
    # Example: Airflow can connect to PostgreSQL using hostname "postgres"
networks:
  weather_network:
    driver: bridge
